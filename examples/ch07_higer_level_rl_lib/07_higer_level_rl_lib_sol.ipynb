{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_higer_level_rl_lib.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOk3ysHOLm/U2wT+rU/Magl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mooithub/pyemotion_rl/blob/master/examples/ch07_higer_level_rl_lib/07_higer_level_rl_lib_sol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKqeZfdBbgBm"
      },
      "source": [
        "# 07. Higher-Level RL Libraries (Solution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjYeJdV9bZ1o"
      },
      "source": [
        "* 파이모션 / Deep RL 핸즈온 [1]\n",
        "* 김무성\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL_E3EN-btrH"
      },
      "source": [
        "# 차례 \n",
        "* Why RL libraries?\n",
        "* The PTAN library\n",
        "* Action selectors\n",
        "* The agent\n",
        "* DQNAgent\n",
        "* PolicyAgent\n",
        "* Experience source\n",
        "* Toy environment\n",
        "* The ExperienceSource class\n",
        "* ExperienceSourceFirstLast\n",
        "* Experience replay buffers\n",
        "* The TargetNet class\n",
        "* The PTAN CartPole solver\n",
        "* Other RL libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apS7osjaE9PC"
      },
      "source": [
        "# 설치"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7yK1cFxGUMI",
        "outputId": "8b237049-f8fc-477b-cf90-6a13f8ed8cca"
      },
      "source": [
        "!pip install ptan==0.6"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ptan==0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/91/cb/57f6d86625f2b24c008b0524ca29559683aa75d00afa38b6b44d7fcad25b/ptan-0.6.tar.gz\n",
            "Collecting torch==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/0b/9d33aef363b6728ad937643d98be713c6c25d50ce338678ad57cee6e6fd5/torch-1.3.0-cp37-cp37m-manylinux1_x86_64.whl (773.1MB)\n",
            "\u001b[K     |████████████████████████████████| 773.1MB 19kB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from ptan==0.6) (0.17.3)\n",
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.7/dist-packages (from ptan==0.6) (0.2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ptan==0.6) (1.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from ptan==0.6) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->ptan==0.6) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->ptan==0.6) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->ptan==0.6) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py->ptan==0.6) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->ptan==0.6) (0.16.0)\n",
            "Building wheels for collected packages: ptan\n",
            "  Building wheel for ptan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ptan: filename=ptan-0.6-cp37-none-any.whl size=23502 sha256=bf1880d34a010149d511f7096b61a84f1153730342c497f5839b7e3101b786a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/4b/2f/9a45fd39b0a614a2716bc6128a7f1adb4647f323a2d90783f2\n",
            "Successfully built ptan\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, ptan\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed ptan-0.6 torch-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQLLbT2QeFox"
      },
      "source": [
        "# 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AamKpnXHuP2"
      },
      "source": [
        "import ptan\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUujdN31GXHo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvKUuzsUc2HZ"
      },
      "source": [
        "-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtYDku4mcrZQ"
      },
      "source": [
        "## Why RL libraries?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzAxXckNc5mS"
      },
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW7TPWcTc6gC"
      },
      "source": [
        "## The PTAN library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efRbLjFRc9p3"
      },
      "source": [
        "-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmdtW_FBGdBD"
      },
      "source": [
        "# Action selectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7hiHkwDHCv7"
      },
      "source": [
        "# All the classes assume that NumPy arrays will be passed to them. The complete example from this section can be found in Chapter07/01_actions.py."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRL_x49EGejM"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15-XNYmbGgmc"
      },
      "source": [
        "import ptan"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aza9H6YOGh-J"
      },
      "source": [
        "q_vals = np.array([[1, 2, 3], [1, -1, 0]])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3CRKOPFGlJK",
        "outputId": "115f5764-5eca-4244-8ee3-7c52ee95d0e7"
      },
      "source": [
        "q_vals"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3],\n",
              "       [ 1, -1,  0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp-_XefbGpMO"
      },
      "source": [
        "selector = ptan.actions.ArgmaxActionSelector()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aMT0y_dGrVO",
        "outputId": "5f8a95a9-ccc6-4b58-ce8b-455b357a5286"
      },
      "source": [
        "selector(q_vals)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiTCwsRRGt4V"
      },
      "source": [
        "# As you can see, the selector returns indices of actions with the largest values."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2FxO4upGu2K"
      },
      "source": [
        "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvlqWhp3GwuR",
        "outputId": "e3aeff6a-29d9-435f-a20a-ae44168d591a"
      },
      "source": [
        "selector(q_vals)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0BRjqFDGyHP"
      },
      "source": [
        "# The result of the EpsilonGreedyActionSelector application is the same, as epsilon is 0.0, which means no random actions are taken. If we change epsilon to 1, actions will be random:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXAAajmEGyaY"
      },
      "source": [
        "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn-UVbOiG2Ca",
        "outputId": "7e02c90c-8dc3-4803-9344-62c26cccbace"
      },
      "source": [
        "selector(q_vals)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTiGOrYMG4zb"
      },
      "source": [
        "# Working with ProbabilityActionSelector is the same, but the input needs to be a normalized probability distribution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M46yDd7JG45d"
      },
      "source": [
        "selector = ptan.actions.ProbabilityActionSelector()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjrniuXkHLhW",
        "outputId": "b007630d-4b6a-42ad-d26d-ec1cdcccbb37"
      },
      "source": [
        "for _ in range(10):\n",
        "  acts = selector(np.array([\n",
        "    [0.1, 0.8, 0.1],\n",
        "    [0.0, 0.0, 1.0],\n",
        "    [0.5, 0.5, 0.0]]))\n",
        "\n",
        "  print(acts)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 0]\n",
            "[1 2 0]\n",
            "[1 2 1]\n",
            "[1 2 0]\n",
            "[0 2 1]\n",
            "[1 2 0]\n",
            "[1 2 1]\n",
            "[1 2 1]\n",
            "[1 2 1]\n",
            "[1 2 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gARpD-xidCTB"
      },
      "source": [
        "-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tifR73fdDOa"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfh5MO1DHR7M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EocYkoCvHTjg"
      },
      "source": [
        "# The agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZdIwtLGdGPN"
      },
      "source": [
        "-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5beW8XSBHWnV"
      },
      "source": [
        "## DQNAgent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqMVOVmcHYYB"
      },
      "source": [
        "class DQNNet(nn.Module):\n",
        "  def __init__(self, actions: int):\n",
        "    super(DQNNet, self).__init__()\n",
        "    self.actions = actions\n",
        "       \n",
        "  def forward(self, x):\n",
        "    return torch.eye(x.size()[0], self.actions)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeK3nWdkHmVS"
      },
      "source": [
        "# Once we have defined the above class, we can use it as a DQN model:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpMRM_w7HzKK"
      },
      "source": [
        "net = DQNNet(actions=3)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKc0GS4uIbkX",
        "outputId": "6e610d36-a4b5-47fe-c380-1a9b843aa31a"
      },
      "source": [
        "net(torch.zeros(2, 10))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [0., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABTEiTycIdtt"
      },
      "source": [
        "# We start with the simple argmax policy, so the agent will always return actions corresponding to 1s in the network output."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy42nDiWIi3y"
      },
      "source": [
        "selector = ptan.actions.ArgmaxActionSelector()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVVu3OiiIks-"
      },
      "source": [
        "agent = ptan.agent.DQNAgent(dqn_model=net, action_selector=selector)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu_NZcvmInDv",
        "outputId": "836d137d-7bdd-448c-ef72-86974115447b"
      },
      "source": [
        "agent(torch.zeros(2, 5))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), [None, None])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2QExfD4IocK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9cO47i4dJ0Q"
      },
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H5atPNedKpK"
      },
      "source": [
        "## PolicyAgent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIKn0cGUeoAS"
      },
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, actions: int):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.actions = actions\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Now we produce the tensor with first two actions\n",
        "        # having the same logit scores\n",
        "        shape = (x.size()[0], self.actions)\n",
        "        res = torch.zeros(shape, dtype=torch.float32)\n",
        "        res[:, 0] = 1\n",
        "        res[:, 1] = 1\n",
        "        return res"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ez06aheuGy",
        "outputId": "b00a18fe-7202-45aa-e15c-69b2ca60a0e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net = PolicyNet(actions=5)\n",
        "net_out = net(torch.zeros(6, 10))\n",
        "print(\"policy_net:\")\n",
        "print(net_out)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "policy_net:\n",
            "tensor([[1., 1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0HjivV-ey_t",
        "outputId": "d816dc10-4ef2-440e-da5e-fecf42b7a53f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "selector = ptan.actions.ProbabilityActionSelector()\n",
        "agent = ptan.agent.PolicyAgent(model=net, action_selector=selector, apply_softmax=True)\n",
        "ag_out = agent(torch.zeros(6, 5))[0]\n",
        "print(ag_out)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 4 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl9pblKXdYnN"
      },
      "source": [
        "-----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCdsPkDhdZjv"
      },
      "source": [
        "## Experience source\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XKiZf8idbJX"
      },
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPhY6ukpdcS8"
      },
      "source": [
        "## Toy environment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR8G_tzCfKqx"
      },
      "source": [
        "import gym\n",
        "import ptan\n",
        "from typing import List, Optional, Tuple, Any"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZDmbyQSfNZt"
      },
      "source": [
        "class ToyEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Environment with observation 0..4 and actions 0..2\n",
        "    Observations are rotated sequentialy mod 5, reward is equal to given action.\n",
        "    Episodes are having fixed length of 10\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ToyEnv, self).__init__()\n",
        "        self.observation_space = gym.spaces.Discrete(n=5)\n",
        "        self.action_space = gym.spaces.Discrete(n=3)\n",
        "        self.step_index = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.step_index = 0\n",
        "        return self.step_index\n",
        "\n",
        "    def step(self, action):\n",
        "        is_done = self.step_index == 10\n",
        "        if is_done:\n",
        "            return self.step_index % self.observation_space.n, \\\n",
        "                   0.0, is_done, {}\n",
        "        self.step_index += 1\n",
        "        return self.step_index % self.observation_space.n, \\\n",
        "               float(action), self.step_index == 10, {}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEe2zr_rfUV0",
        "outputId": "8761649b-0dfe-4442-af29-44a0be60399a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = ToyEnv()\n",
        "s = env.reset()\n",
        "print(\"env.reset() -> %s\" % s)\n",
        "s = env.step(1)\n",
        "print(\"env.step(1) -> %s\" % str(s))\n",
        "s = env.step(2)\n",
        "print(\"env.step(2) -> %s\" % str(s))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env.reset() -> 0\n",
            "env.step(1) -> (1, 1.0, False, {})\n",
            "env.step(2) -> (2, 2.0, False, {})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za0emAwafcAK",
        "outputId": "3ea53497-9f94-4f9b-e28c-a9362d87f584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for _ in range(10):\n",
        "  r = env.step(0)\n",
        "  print(r)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 0.0, False, {})\n",
            "(4, 0.0, False, {})\n",
            "(0, 0.0, False, {})\n",
            "(1, 0.0, False, {})\n",
            "(2, 0.0, False, {})\n",
            "(3, 0.0, False, {})\n",
            "(4, 0.0, False, {})\n",
            "(0, 0.0, True, {})\n",
            "(0, 0.0, True, {})\n",
            "(0, 0.0, True, {})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE0PRsNHf3IG"
      },
      "source": [
        "class DullAgent(ptan.agent.BaseAgent):\n",
        "    \"\"\"\n",
        "    Agent always returns the fixed action\n",
        "    \"\"\"\n",
        "    def __init__(self, action: int):\n",
        "        self.action = action\n",
        "\n",
        "    def __call__(self, observations: List[Any],\n",
        "                 state: Optional[List] = None) \\\n",
        "            -> Tuple[List[int], Optional[List]]:\n",
        "        return [self.action for _ in observations], state"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XLAnPEZf40c",
        "outputId": "cf04fa1f-4759-46d0-b770-353da4e3f35e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "agent = DullAgent(action=1)\n",
        "print(\"agent:\", agent([1, 2])[0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "agent: [1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkmbcVTtdey3"
      },
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYWL_wu-f8sv",
        "outputId": "b9dd3c25-b565-4a62-e521-500c3ed89301",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = ToyEnv()\n",
        "agent = DullAgent(action=1)\n",
        "exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=2)\n",
        "for idx, exp in enumerate(exp_source):\n",
        "  if idx > 15:\n",
        "      break\n",
        "  print(exp)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
            "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
            "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
            "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
            "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
            "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
            "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
            "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
            "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=True))\n",
            "(Experience(state=4, action=1, reward=1.0, done=True),)\n",
            "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
            "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
            "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
            "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
            "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
            "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hzWeCpPdeGc"
      },
      "source": [
        "## The ExperienceSource class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eoaNrRggHpz",
        "outputId": "6fc235ef-3af3-4bed-df22-bab374dedea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=4)\n",
        "print(next(iter(exp_source)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Y3Wmw9g-_0",
        "outputId": "2e2fcffd-4c3d-4a5c-c97b-e1c54ad0213f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "exp_source = ptan.experience.ExperienceSource(env=[ToyEnv(), ToyEnv()], agent=agent, steps_count=2)\n",
        "for idx, exp in enumerate(exp_source):\n",
        "  if idx > 4:\n",
        "      break\n",
        "  print(exp)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
            "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
            "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
            "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
            "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcmmEh38dhD7"
      },
      "source": [
        "---------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9yiyV0TdiFg"
      },
      "source": [
        "## ExperienceSourceFirstLast\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdvl0EMuhD2a",
        "outputId": "b8e37c9f-228f-4a6e-fe5d-aa547125782e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"ExperienceSourceFirstLast\")\n",
        "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=1)\n",
        "for idx, exp in enumerate(exp_source):\n",
        "  print(exp)\n",
        "  if idx > 10:\n",
        "      break"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ExperienceSourceFirstLast\n",
            "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
            "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
            "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
            "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
            "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
            "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
            "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
            "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
            "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
            "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
            "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
            "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiAigcUudsK2"
      },
      "source": [
        "-----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idXUffQ5dtNc"
      },
      "source": [
        "## Experience replay buffers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDxrXZdHhbjP"
      },
      "source": [
        "env = ToyEnv()\n",
        "agent = DullAgent(action=1)\n",
        "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=1)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFvm6SmzhmXd"
      },
      "source": [
        "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=100)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hri7EMPbhomy",
        "outputId": "891f0c18-a590-4323-915f-48cb7ba8778b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for step in range(6):\n",
        "    buffer.populate(1)\n",
        "    # if buffer is small enough, do nothing\n",
        "    if len(buffer) < 5:\n",
        "        continue\n",
        "    batch = buffer.sample(4)\n",
        "    print(\"Train time, %d batch samples:\" % len(batch))\n",
        "    for s in batch:\n",
        "        print(s)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train time, 4 batch samples:\n",
            "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
            "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
            "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
            "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
            "Train time, 4 batch samples:\n",
            "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
            "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
            "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
            "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5NAv7obduap"
      },
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvjkqr5NdvRq"
      },
      "source": [
        "## The TargetNet class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9aQLnV4iCD8"
      },
      "source": [
        "import ptan\n",
        "import torch.nn as nn"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thii9pKYh_fm"
      },
      "source": [
        "class DQNNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DQNNet, self).__init__()\n",
        "        self.ff = nn.Linear(5, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ff(x)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vut-WIKiFt1",
        "outputId": "d25aa38c-40e0-4493-ceb4-c728f1a9d8c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net = DQNNet()\n",
        "print(net)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DQNNet(\n",
            "  (ff): Linear(in_features=5, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj1vjBhsiN2B",
        "outputId": "2d8bb1b4-4ee4-4c30-9759-5dbb247b07f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tgt_net = ptan.agent.TargetNet(net)\n",
        "print(\"Main net:\", net.ff.weight)\n",
        "print(\"Target net:\", tgt_net.target_model.ff.weight)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Main net: Parameter containing:\n",
            "tensor([[ 0.1032, -0.0820, -0.1103,  0.1248,  0.2950],\n",
            "        [-0.3504, -0.1226, -0.2608, -0.4029,  0.1985],\n",
            "        [ 0.1090, -0.3073,  0.1705, -0.2395,  0.4092]], requires_grad=True)\n",
            "Target net: Parameter containing:\n",
            "tensor([[ 0.1032, -0.0820, -0.1103,  0.1248,  0.2950],\n",
            "        [-0.3504, -0.1226, -0.2608, -0.4029,  0.1985],\n",
            "        [ 0.1090, -0.3073,  0.1705, -0.2395,  0.4092]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-YWYUhOiPs3",
        "outputId": "b89ae48b-3285-4d77-829d-39cb7b901fcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net.ff.weight.data += 1.0\n",
        "print(\"After update\")\n",
        "print(\"Main net:\", net.ff.weight)\n",
        "print(\"Target net:\", tgt_net.target_model.ff.weight)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After update\n",
            "Main net: Parameter containing:\n",
            "tensor([[1.1032, 0.9180, 0.8897, 1.1248, 1.2950],\n",
            "        [0.6496, 0.8774, 0.7392, 0.5971, 1.1985],\n",
            "        [1.1090, 0.6927, 1.1705, 0.7605, 1.4092]], requires_grad=True)\n",
            "Target net: Parameter containing:\n",
            "tensor([[ 0.1032, -0.0820, -0.1103,  0.1248,  0.2950],\n",
            "        [-0.3504, -0.1226, -0.2608, -0.4029,  0.1985],\n",
            "        [ 0.1090, -0.3073,  0.1705, -0.2395,  0.4092]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewitnom5iSE6",
        "outputId": "f6ac2d9f-c9d1-43c0-a931-bfc87c3c7ba7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tgt_net.sync()\n",
        "print(\"After sync\")\n",
        "print(\"Main net:\", net.ff.weight)\n",
        "print(\"Target net:\", tgt_net.target_model.ff.weight)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After sync\n",
            "Main net: Parameter containing:\n",
            "tensor([[1.1032, 0.9180, 0.8897, 1.1248, 1.2950],\n",
            "        [0.6496, 0.8774, 0.7392, 0.5971, 1.1985],\n",
            "        [1.1090, 0.6927, 1.1705, 0.7605, 1.4092]], requires_grad=True)\n",
            "Target net: Parameter containing:\n",
            "tensor([[1.1032, 0.9180, 0.8897, 1.1248, 1.2950],\n",
            "        [0.6496, 0.8774, 0.7392, 0.5971, 1.1985],\n",
            "        [1.1090, 0.6927, 1.1705, 0.7605, 1.4092]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou4HdpXkdwup"
      },
      "source": [
        "-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Oz7JnJdxl3"
      },
      "source": [
        "## The PTAN CartPole solver\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvgct6RDiejo"
      },
      "source": [
        "* https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/blob/3ebbd9cab1e936a05a1e8c5b384d552e6819e7a9/Chapter07/06_cartpole.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIp2D9pudyuR"
      },
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLdwQakhdzwO"
      },
      "source": [
        "## Other RL libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAUOi2tBd0dn"
      },
      "source": [
        "---------------------\n",
        "참고자료\n",
        "* [1] Deep Reinforcement Learning Hands-On\n",
        "  - 책 - https://www.amazon.com/dp/B076H9VQH6/\n",
        "  - github - https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
      ]
    }
  ]
}